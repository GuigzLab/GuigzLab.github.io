---
layout: distill
title: Image Captioning
description: A project in which I implemented a Neural Image Caption (NIC) network to generate image captions
img: assets/img/12.jpg
importance: 1
category: school
published: false

authors:
  - name: Guillaume Dufau
    url: "https://guigzlab.github.io/"
    affiliations:
      name: Université Paris Cité

toc:
  - name: Motivation
  - name: Methodology
    subsections:
      - name: Dataset
      - name: Model
      - name: Training
  - name: Évaluation

bibliography: nic-project.bib
# bibliography: 2018-12-22-distill.bib

---

# Motivation

Automatically describing the content of an image, or "Image Captioning," is a major task in computer vision and natural language processing. It involves generating a textual description that reflects the content of a given image. As part of my second year of Master's degree, I developed a project where I implemented a NIC (Neural Image Captioning) neural network, leveraging the model proposed in the influential _"Show and Tell"_ paper proposed by Vinyals, Oriol et al. <d-cite key="Vinyals2014ShowAT"></d-cite>

# Neural Image Caption (NIC)

Le modèle NIC est un réseau de neurones profond, basé sur les modèles de traduction automatique, qui utilise une architecture encodeur-décodeur. L'encodeur est un réseau de neurones convolutionnel (CNN) qui transforme l'image en une représentation vectorielle. Le décodeur, un réseau de neurones récurrent (RNN), utilise cette représentation pour générer une séquence de mots décrivant l'image.

<img src="/assets/img/nic-model.png" style="width:100%;">

# Implementation

## Dataset

Le dataset utilisé dans ce projet est Flickr8k, une collection de 8000 images, chacune associée à cinq légendes différentes provenant de six groupes [Flickr](https://www.flickr.com/) différents. Les images ont été sélectionnées manuellement pour représenter une variété de scènes et de situations.

Ullistration d'images du dataset ainsi que des légendes associées :
<img src="/assets/img/flickr8k-example.png" style="width:100%;">

## CNN Encoder

L'architecture présentée dans l'article est indépendante du choix du CNN. Ici, j'ai utilisé DenseNet201 chargé avec les poids d'ImageNet afin d'extraire les caractéristiques de l'image en amont, dans le but d'alléger l'entraînement.

Illustration d'un _DenseBlock_, élément clé de DenseNet :
<div style="text-align: center;">
  <img src="/assets/img/denseblock.png" style="width:65%; margin: 10px auto; position: relative;">
</div>

## RNN Decoder

Ici, le décodeur utilisé est un _Long Short-Term Memory (LSTM)_. Son rôle est de décoder le vecteur de caractéristiques obtenu à partir du CNN en une légende. Il est entraîné pour prédire chaque mot de la légende après avoir vu l'image et tous les mots précédents.

Illustration du réseau final obtenu :
<img src="/assets/img/nic-plot.png" style="width:100%; margin: 20px auto;">

## Inference

On peut utiliser plusieurs méthodes différentes pour décoder la légende générée par le réseau :

- Sampling : Cette méthode génère le prochain mot en échantillonnant aléatoirement selon la distribution de probabilité des mots prédits par le modèle. Cela peut conduire à des résultats plus diversifiés, mais parfois moins cohérents.
- Beam Search : Cette méthode conserve les k meilleures légendes à chaque étape de prédiction. À chaque nouveau mot, elle élargit ces k légendes et ne conserve que les k meilleures parmi elles. Cela permet d'obtenir des résultats plus précis que le sampling ou la recherche gloutonne, mais est plus coûteux en termes de calcul.
- Greedy Search : Cette méthode choisit toujours le mot avec la plus haute probabilité comme prochain mot dans la légende. C'est l'approche la plus rapide et la plus simple, mais elle peut parfois conduire à des résultats sous-optimaux car elle ne prend pas en compte les probabilités des mots suivants. On peut résumer Greedy Search par un Beam Search avec `k = 1`)

Ici, j'ai choisi d'utiliser Greedy Search, car c'est un bon compromis entre performances et temps d'implémentation.

Illustration de Greedy Search : 
<div style="text-align: center;">
  <img src="/assets/img/greedy-search.gif" style="width:60%; margin: -10px auto 20px auto;">
</div>

# Evaluation

## Examples

Before diving into the metrics, here are a few examples of captions generated by NIC.

<img src="/assets/img/nic-inference.png" style="width:100%; margin: 10px auto">

## Metrics

Avant de présenter les résultats, il est important de comprendre les différentes métriques utilisées pour évaluer la qualité des descriptions générées :

- **BLEU (Bilingual Evaluation Understudy)** 1 à 4 :
  - Mesure la correspondance entre les phrases générées et les phrases de référence.
  - Considère l'exactitude des n-grammes<d-footnote>Les n-grammes sont des groupes de "n" mots consécutifs dans un texte. Par exemple, dans "J'aime la science", un 2-gramme (ou bigramme) serait ["J'aime la", "la science"]</d-footnote>
 (de 1 à 4 mots).
- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** :
  - Mesure la capacité du modèle à inclure des informations clés.
  - Focalisé sur le rappel des n-grammes.
- **METEOR (Metric for Evaluation of Translation with Explicit ORdering)** :
  - Compare le texte généré avec les références en tenant compte de la synonymie, des variations morphologiques, et de l'ordre des mots.
  - A pour objectif d'améliorer la corrélation avec l'évaluation humaine.



## Scores

<div style="text-align: center;">
  <img src="/assets/img/nic-scores.png" style="width:60%; margin: -10px auto 20px auto;">
</div>

## Comparison with X

# Conclusion

Dans ce projet, j'ai pu implémenter un des modèles fondateurs dans le domaine de la génération de légendes automatique, en obtenant des résultats satisfaisants. Le modèle semble avoir une compréhension relative des images, tout en ayant quelques problèmes récurrents comme compter les personnes ou les animaux, une sur utilisation de _"in a blue shirt"_ pour d'autres couleurs de vêtements, etc.

Dans le futur, il serait intéressant de se pencher sur certains points afin d'améliorer le modèle : 
- Changer l'architecture pour utiliser les mécanismes d'attention comme dans "Show, Attend and Tell" de Xu, Ke et al. <d-cite key="Xu2015ShowAA"></d-cite>
- Utiliser un dataset d'entraînement plus large
- Utiliser un encodeur et/ou un décodeur pré-entraîné sur une plus grande base
- Utiliser Beam search avec `k > 1`pour l'inférence