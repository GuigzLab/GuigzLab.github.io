<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Image Captioning | Guillaume  Dufau</title>
    <meta name="author" content="Guillaume  Dufau">
    <meta name="description" content="A project in which I implemented a Neural Image Caption (NIC) network to generate image captions">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8D&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://guigzlab.github.io/projects/image-captioning/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Image Captioning",
      "description": "A project in which I implemented a Neural Image Caption (NIC) network to generate image captions",
      "published": "June 6, 2023",
      "authors": [
        {
          "author": "Guillaume Dufau",
          "authorURL": "https://guigzlab.github.io/",
          "affiliations": [
            {
              "name": "Université Paris Cité",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Guillaume </span>Dufau</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li> -->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Image Captioning</h1>
        <p>A project in which I implemented a Neural Image Caption (NIC) network to generate image captions</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#motivation">Motivation</a></div>
            <div><a href="#neural-image-caption-nic">Neural Image Caption (NIC)</a></div>
            <div><a href="#implementation">Implementation</a></div>
            <ul>
              <li><a href="#dataset">Dataset</a></li>
              <li><a href="#cnn-encoder">CNN Encoder</a></li>
              <li><a href="#rnn-decoder">RNN Decoder</a></li>
              <li><a href="#inference">Inference</a></li>
              
            </ul>
<div><a href="#evaluation">Evaluation</a></div>
            <ul>
              <li><a href="#examples">Examples</a></li>
              <li><a href="#metrics">Metrics</a></li>
              <li><a href="#scores">Scores</a></li>
              <li><a href="#comparison">Comparison</a></li>
              
            </ul>
<div><a href="#conclusion">Conclusion</a></div>
            
          </nav>
        </d-contents>

        <h1 id="motivation">Motivation</h1>

<p>Automatically describing the content of an image, or “Image Captioning,” is a major task in computer vision and natural language processing. It involves generating a textual description that reflects the content of a given image. As part of my second year of Master’s degree, I developed a project where I implemented a NIC (Neural Image Captioning) neural network, leveraging the model proposed in the influential <em>“Show and Tell”</em> paper proposed by Vinyals, Oriol et al. <d-cite key="Vinyals2014ShowAT"></d-cite></p>

<h1 id="neural-image-caption-nic">Neural Image Caption (NIC)</h1>

<p>The NIC model is a deep neural network, based on machine translation models, that utilizes an encoder-decoder architecture. The encoder is a convolutional neural network (CNN) that transforms the image into a vector representation. The decoder, a recurrent neural network (RNN), uses this representation to generate a sequence of words describing the image.</p>

<p><img src="/assets/img/nic-model.png" style="width:100%;"></p>

<h1 id="implementation">Implementation</h1>

<h2 id="dataset">Dataset</h2>

<p>The dataset used in this project is Flickr8k, a collection of 8000 images, each associated with five different captions from six different <a href="https://www.flickr.com/" rel="external nofollow noopener" target="_blank">Flickr</a> groups. The images have been manually selected to represent a variety of scenes and situations.</p>

<p>Illustration of images from the dataset along with their associated captions:
<img src="/assets/img/flickr8k-example.png" style="width:100%;"></p>

<h2 id="cnn-encoder">CNN Encoder</h2>

<p>The architecture presented in the article is independent of the choice of CNN. Here, I used <em>DenseNet201</em> loaded with the <em>ImageNet</em> weights to extract image features upstream, with the goal of reducing the training burden.</p>

<p>Illustration of a <em>DenseBlock</em>, a key element of DenseNet:</p>
<div style="text-align: center;">
  <img src="/assets/img/denseblock.png" style="width:65%; margin: 10px auto; position: relative;">
</div>

<h2 id="rnn-decoder">RNN Decoder</h2>

<p>Here, the decoder used is a <em>Long Short-Term Memory (LSTM)</em>. Its role is to decode the feature vector obtained from the CNN into a caption. It is trained to predict each word of the caption after seeing the image and all the previous words.</p>

<p>Illustration of the final network obtained:
<img src="/assets/img/nic-plot.png" style="width:100%; margin: 20px auto"></p>

<h2 id="inference">Inference</h2>

<p>Many different methods can be used to decode the caption generated by NIC:</p>

<ul>
  <li>Sampling: This method generates the next word by randomly sampling from the probability distribution of the words predicted by the model. This can lead to more diverse results, but sometimes less coherent.</li>
  <li>Beam Search: This method keeps the top <code class="language-plaintext highlighter-rouge">k</code> captions at each prediction step. With each new word, it expands these <code class="language-plaintext highlighter-rouge">k</code> captions and retains only the top <code class="language-plaintext highlighter-rouge">k</code> among them. This allows for more accurate results than sampling or greedy search, but is more computationally expensive.</li>
  <li>Greedy Search: This method always chooses the word with the highest probability as the next word in the caption. It is the fastest and simplest approach but can sometimes result in suboptimal results as it does not consider probabilities of subsequent words. Greedy search can be seen as a special case of beam search with <code class="language-plaintext highlighter-rouge">k = 1</code>.</li>
</ul>

<p>Here, I chose to use Greedy Search as it is a good compromise between performance and implementation time.</p>

<p>Illustration of Greedy Search:</p>
<div style="text-align: center;">
  <img src="/assets/img/greedy-search.gif" style="width:50%; margin: -10px auto 20px auto;">
</div>

<p><br></p>
<h1 id="evaluation">Evaluation</h1>

<h2 id="examples">Examples</h2>

<p>Before diving into the metrics, here are a few examples of captions generated by NIC.</p>

<p><img src="/assets/img/nic-inference.png" style="width:100%; margin: 10px auto"></p>

<h2 id="metrics">Metrics</h2>

<p>Before presenting the results, it is important to understand the different metrics used to evaluate the quality of the generated captions:</p>

<ul>
  <li>
<strong>BLEU (Bilingual Evaluation Understudy)</strong> 1 to 4:
    <ul>
      <li>Measures the correspondence between the generated sentences and the reference sentences.</li>
      <li>Considers the accuracy of n-grams<d-footnote>N-grams are groups of "n" consecutive words in a text. For example, in "I love science", a 2-gram (or bigram) would be ["I love", "love science"].</d-footnote> (from 1 to 4 words).</li>
    </ul>
  </li>
  <li>
<strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong>:
    <ul>
      <li>Measures the model’s ability to include key words.</li>
      <li>Focuses on the recall of n-grams.</li>
    </ul>
  </li>
  <li>
<strong>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</strong>:
    <ul>
      <li>Compares the generated text with the references, taking into account synonymy, morphological variations, and word order.</li>
      <li>Aims to improve correlation with human evaluation.</li>
    </ul>
  </li>
</ul>

<h2 id="scores">Scores</h2>

<p>The obtained scores are correct, and it can be observed that the model makes sense of most of the images. For reference, human BLEU and ROUGE scores are approximately around 70%, so higher scores would be unlikely.</p>

<div style="text-align: center;">
  <img src="/assets/img/nic-scores.png" style="width:60%; margin: 0px auto 10px auto;">
</div>

<h2 id="comparison">Comparison</h2>

<p>Here are the results from an implementation conducted by Arthur Benard, who utilized the encoder portion of CLIP<d-cite key="Radford2021LearningTV"></d-cite>, from OpenAI.</p>

<div style="text-align: center;">
  <img src="/assets/img/nic-clip.png" style="width:62%; margin: 0px auto 10px auto;">
</div>

<p>We can notice that the results are slightly different, with lower BLEU scores, equivalent ROUGE scores, and higher METEOR scores. This could indicate that the generated captions may have a different structure but keep a meaning closer to what a human would describe.</p>

<h1 id="conclusion">Conclusion</h1>

<p>In this project, I was able to implement one of the foundation models in the field of image captioning, achieving correct results. The model appears to have a relative understanding of the images, while having some recurring issues such as counting people or animals, overusing “in a blue shirt” for other clothing colors, etc.</p>

<p>In the future, it would be interesting to explore certain points to improve the model:</p>
<ul>
  <li>Changing the architecture to incorporate attention mechanisms, as in “Show, Attend and Tell” by Xu, Ke et al. <d-cite key="Xu2015ShowAA"></d-cite>
</li>
  <li>Using a larger training dataset such as Flickr30k<d-cite key="Young2014FromID"></d-cite> or MS COCO<d-cite key="Lin2014MicrosoftCC"></d-cite>
</li>
  <li>Employing an encoder and/or decoder pre-trained on a larger corpus</li>
  <li>Using Beam search with <code class="language-plaintext highlighter-rouge">k &gt; 1</code> for inference</li>
</ul>

<p><br></p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/nic-project.bib"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0" style="text-align: center;">
        © Copyright 2023 Guillaume  Dufau. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
